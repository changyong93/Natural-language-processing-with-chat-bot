{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "딥러닝을_이용한_자연어처리_입문(12_5_NLP를 위한 신경망(CNN)_Multi-Kernel 1D CNN으로 네이버 영화 리뷰 분류하기",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyN5DPZvktb23dHNI5ieB3Tv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/changyong93/Natural-language-processing-with-chat-bot/blob/main/%EB%94%A5%EB%9F%AC%EB%8B%9D%EC%9D%84_%EC%9D%B4%EC%9A%A9%ED%95%9C_%EC%9E%90%EC%97%B0%EC%96%B4%EC%B2%98%EB%A6%AC_%EC%9E%85%EB%AC%B8(12_5_NLP%EB%A5%BC_%EC%9C%84%ED%95%9C_%EC%8B%A0%EA%B2%BD%EB%A7%9D(CNN)_Multi_Kernel_1D_CNN%EC%9C%BC%EB%A1%9C_%EB%84%A4%EC%9D%B4%EB%B2%84_%EC%98%81%ED%99%94_%EB%A6%AC%EB%B7%B0_%EB%B6%84%EB%A5%98%ED%95%98%EA%B8%B0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6yLB78V2Uuvd"
      },
      "source": [
        "# Multi-Kernel 1D CNN으로 네이버 영화 리뷰 분류하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zaaem1BYVNKV"
      },
      "source": [
        "!pip install konlpy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4i5OW69RVBwd"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import urllib.request\n",
        "\n",
        "from konlpy.tag import Okt\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2LzL-MVdVcmw"
      },
      "source": [
        "## 데이터 로드"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OOwUPdD2VhD8"
      },
      "source": [
        "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt\", filename=\"ratings_train.txt\")\n",
        "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt\", filename=\"ratings_test.txt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VtY1WMcSVoZ_"
      },
      "source": [
        "train_data = pd.read_table('ratings_train.txt')\n",
        "test_data = pd.read_table('ratings_test.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tgiHhib_Vu5L"
      },
      "source": [
        "print(\"훈련용 데이터 개수: \", train_data.shape[0])\n",
        "print(\"테스트용 데이터 개수: \", test_data.shape[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mCHCUCKCV5w-"
      },
      "source": [
        "train_data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mza6S2AVV8os"
      },
      "source": [
        "test_data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g9BjRN-UWABF"
      },
      "source": [
        "## 데이터 정제"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OIexr-JEWCo5"
      },
      "source": [
        "train_data.nunique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3bWF2E7gWIOP"
      },
      "source": [
        "train_data = train_data.drop_duplicates(subset = [\"document\"])\n",
        "train_data.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-rQTkh0WSrP"
      },
      "source": [
        "train_data.label.value_counts().reset_index(name = \"count\")\n",
        "train_data.label.value_counts().plot.bar()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6R-Z88d8WqBW"
      },
      "source": [
        "train_data.isnull().any()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HCAXMwtLWtmX"
      },
      "source": [
        "train_data.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fpBzCallWv-U"
      },
      "source": [
        "train_data[train_data.document.isnull()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hdD63x83W0sN"
      },
      "source": [
        "train_data = train_data.dropna(how = 'any')\n",
        "train_data.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E42ipgNqW8z2"
      },
      "source": [
        "train_data[\"document\"] = train_data.document.str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")\n",
        "train_data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPvsxrxFXuRV"
      },
      "source": [
        "train_data[\"document\"] = train_data[\"document\"].str.replace(\"^ +\", \"\")\n",
        "train_data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "boVmaJ1OX7bm"
      },
      "source": [
        "train_data[\"document\"] = train_data[\"document\"].replace(\"\",np.nan)\n",
        "train_data.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yYbFeOVSYmSe"
      },
      "source": [
        "train_data[train_data[\"document\"].isnull()].head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JwGCd4RhY422"
      },
      "source": [
        "train_data = train_data.dropna(how = 'any')\n",
        "train_data.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kM_g7o7CZA-L"
      },
      "source": [
        "print(test_data.shape)\n",
        "test_data = test_data.drop_duplicates(subset = ['document'])\n",
        "test_data[\"document\"] = test_data[\"document\"].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\", \"\")\n",
        "test_data[\"document\"] = test_data[\"document\"].str.replace(\"^ +\", \"\")\n",
        "test_data[\"document\"] = test_data[\"document\"].replace(\"\", np.nan)\n",
        "test_data = test_data.dropna(how = 'any')\n",
        "print(test_data.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NrD6trjFZdbc"
      },
      "source": [
        "## 토큰화"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cuoxpKrzaq4i"
      },
      "source": [
        "okt = Okt()\n",
        "train_data[\"tokenized\"] = train_data[\"document\"].apply(lambda x : okt.morphs(x,stem = True))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j7TXgRTycDHT"
      },
      "source": [
        "stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']\n",
        "train_data[\"tokenized\"] = train_data[\"tokenized\"].map(lambda x : [word for word in x if not word in stopwords])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UBd8WTIGdqC2"
      },
      "source": [
        "train_data[\"tokenized\"][:5].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dkE6pjR7dw6j"
      },
      "source": [
        "test_data[\"tokenized\"] = test_data[\"document\"].apply(lambda x : okt.morphs(x,stem = True))\n",
        "test_data[\"tokenized\"] = test_data[\"tokenized\"].map(lambda x : [word for word in x if not word in stopwords])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dAFVRWurd9QA"
      },
      "source": [
        "test_data[\"tokenized\"][:5].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykvKy6vgd_9V"
      },
      "source": [
        "## 정수인코딩"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J94XjNh1eCYl"
      },
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(train_data[\"tokenized\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-awViWEeMsV"
      },
      "source": [
        "print(tokenizer.word_index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QiDYOIaReQAP"
      },
      "source": [
        "threshold = 3\n",
        "total_cnt = len(tokenizer.word_index)\n",
        "total_freq = 0\n",
        "\n",
        "rare_cnt = 0\n",
        "rare_freq = 0\n",
        "for key,value in tokenizer.word_counts.items():\n",
        "  total_freq += value\n",
        "  if value < 3:\n",
        "    rare_cnt += 1\n",
        "    rare_freq += value\n",
        "\n",
        "print(\"단어 집합 크기: \",total_cnt)\n",
        "print(f\"등반 빈도가 {threshold} 미만인 희귀 단어 수: {rare_cnt}\")\n",
        "print(f\"전체 단어 중 희귀 단어 비율: {rare_cnt / total_cnt * 100:.3f}\")\n",
        "print(f\"전체 등장 빈도 중 희귀 단어 비율: {rare_freq / total_freq * 100:.3f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-wmjmCe3JID"
      },
      "source": [
        "vocab_size = total_cnt - rare_cnt + 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BQCAvikd3SYj"
      },
      "source": [
        "vocab_size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1qIViu9K3S_l"
      },
      "source": [
        "tokenizer = Tokenizer(num_words = vocab_size)\n",
        "tokenizer.fit_on_texts(train_data[\"tokenized\"])\n",
        "\n",
        "X_train = tokenizer.texts_to_sequences(train_data[\"tokenized\"])\n",
        "X_test = tokenizer.texts_to_sequences(test_data[\"tokenized\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F6fDRyiX3nbH"
      },
      "source": [
        "y_train = np.array(train_data[\"label\"])\n",
        "y_test = np.array(test_data[\"label\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDQ1u77043FT"
      },
      "source": [
        "## 빈 샘플 제거"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Dwk6JOq4398"
      },
      "source": [
        "drop_train = [idx for idx, sentence in enumerate(X_train) if len(sentence) < 1]\n",
        "\n",
        "#빈 샘플 제거\n",
        "X_train = np.delete(X_train, drop_train, axis = 0)\n",
        "y_train = np.delete(y_train, drop_train, axis = 0)\n",
        "\n",
        "print(len(X_train), len(y_train))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EMmb8Sqr5-5b"
      },
      "source": [
        "## 패딩"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kWCwbvQG6SFX"
      },
      "source": [
        "max_len = max([len(s) for s in X_train])\n",
        "len_list = [len(s) for s in X_train]\n",
        "for length in range(1,max_len+1):\n",
        "  cnt = 0\n",
        "  for s in X_train:\n",
        "    if len(s) <= length:\n",
        "      cnt +=1\n",
        "  print(f\"샘플 중 길이가 {length} 이하인 샘플의 비율: {cnt / len(len_list)*100:.3f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BoLoqcRO7tPn"
      },
      "source": [
        "max_len = 30\n",
        "X_train = pad_sequences(sequences=X_train, maxlen = max_len)\n",
        "X_test = pad_sequences(sequences=X_test, maxlen = max_len)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VjoZ5WFG71wU"
      },
      "source": [
        "## Multi-Kernel 1D CNN으로 네이버 영화 리뷰 분류하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K7G6ukgp7_rY"
      },
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.layers import Dense, Embedding, GlobalMaxPooling1D, Dropout, Conv1D, Input, Flatten, Concatenate\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8B1rbnR185cq"
      },
      "source": [
        "# 하이퍼 파라미터 정의\n",
        "embedding_dim = 128\n",
        "dropout_porb = (0,5, 0.8)\n",
        "num_filters = 128"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MabOjNDn8-lX"
      },
      "source": [
        "# 입력층과 임베딩층 정의\n",
        "# 임베딩층 이후 50% 드랍아웃\n",
        "\n",
        "model_input = Input(shape = (max_len,))\n",
        "z = Embedding(input_dim = vocab_size, output_dim = embedding_dim,input_length = max_len, name = \"embedding\")(model_input)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}